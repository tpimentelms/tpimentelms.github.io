pub_date	title	authors	venue	excerpt	citation	url_slug	paper_url	abstract	bibtex	bibtype	bibid	year	pages	doi	volume	number	highlights
2013-03-02	Mapping planetary caves with an autonomous, heterogeneous robot team	Ammar Husain, Heather Jones, Balajee Kannan, Uland Wong, Tiago Pimentel, Sarah Tang, Shreyansh Daftry, Steven Huber, William L. Whittaker	IEEE Aerospace Conference	This paper is about multi-robot exploration. It presents a framework for coordinating and allocating tasks to an heterogeneous group of robots.	Husain A, Jones H, Kannan B, Wong U, Pimentel T, Tang S, Daftry S, Huber S, Whittaker WL. Mapping planetary caves with an autonomous, heterogeneous robot team. In: IEEE Aerospace Conference, 2013 Mar 2 (pp. 1-13). IEEE.	mapping-planetary-caves	https://ieeexplore.ieee.org/document/6497363	Caves on other planetary bodies offer sheltered habitat for future human explorers and numerous clues to a planet's past for scientists. While recent orbital imagery provides exciting new details about cave entrances on the Moon and Mars, the interiors of these caves are still unknown and not observable from orbit. Multi-robot teams offer unique solutions for exploration and modeling subsurface voids during precursor missions. Robot teams that are diverse in terms of size, mobility, sensing, and capability can provide great advantages, but this diversity, coupled with inherently distinct low-level behavior architectures, makes coordination a challenge. This paper presents a framework that consists of an autonomous frontier and capability-based task generator, a distributed market-based strategy for coordinating and allocating tasks to the different team members, and a communication paradigm for seamless interaction between the different robots in the system. Robots have different sensors, (in the representative robot team used for testing: 2D mapping sensors, 3D modeling sensors, or no exteroceptive sensors), and varying levels of mobility. Tasks are generated to explore, model, and take science samples. Based on an individual robot's capability and associated cost for executing a generated task, a robot is autonomously selected for task execution. The robots create coarse online maps and store collected data for high resolution offline modeling. The coordination approach has been field tested at a mock cave site with highly-unstructured natural terrain, as well as an outdoor patio area. Initial results are promising for applicability of the proposed multi-robot framework to exploration and modeling of planetary caves.	@INPROCEEDINGS{husain2013mapping,   author={     Ammar Husain and     Heather Jones and     Balajee Kannan and     Uland Wong and     Tiago Pimentel and     Sarah Tang and     Shreyansh Daftry and     Steven Huber and     William L. Whittaker   },   booktitle={2013 IEEE Aerospace Conference},   title={Mapping planetary caves with an autonomous, heterogeneous robot team},   year={2013},   volume={},   number={},   pages={1-13}, }	inproceedings	husain2013mapping	2013	1-13				
2018-01-01	Detection of bimanual gestures everywhere: Why it matters, what we need and what is missing	Divya Shah, Ernesto Denicia, Tiago Pimentel, Barbara Bruno, Fulvio Mastrogiovanni	Robotics and Autonomous Systems		Shah D, Denicia E, Pimentel T, Bruno B, Mastrogiovanni F. Detection of bimanual gestures everywhere: Why it matters, what we need and what is missing. In: Robotics and Autonomous Systems. 2018 99:30-49.	detection-of-bimanual-gestures	http://www.sciencedirect.com/science/article/pii/S0921889016303773	Bimanual gestures are of the utmost importance for the study of motor coordination in humans and in everyday activities. A reliable detection of bimanual gestures in unconstrained environments is fundamental for their clinical study and to assess common activities of daily living. This paper investigates techniques for a reliable, unconstrained detection and classification of bimanual gestures. The work assumes the availability of inertial data originating from the two hands/arms, builds upon a previously developed technique for gesture modeling based on Gaussian Mixture Modeling (GMM) and Gaussian Mixture Regression (GMR), and compares different modeling and classification techniques, which are based on a number of assumptions inspired by literature about how bimanual gestures are represented and modeled in the brain. Experiments show results related to 5 everyday bimanual activities, which have been selected on the basis of three main parameters: (not) constraining the two hands by a physical tool, (not) requiring a specific sequence of single-hand gestures, being recursive (or not). In the best performing combination of modeling approach and classification technique, we achieve overall accuracy, precision, recall and F1-score above 80%.	@article{shah2018detection, title = "Detection of bimanual gestures everywhere: {W}hy it matters, what we need and what is missing", journal = "Robotics and Autonomous Systems", volume = "99", pages = "30 - 49", year = "2018", issn = "0921-8890", doi = "https://doi.org/10.1016/j.robot.2017.09.016", url = "http://www.sciencedirect.com/science/article/pii/S0921889016303773", author = "Divya Shah and Ernesto Denicia and Tiago Pimentel and Barbara Bruno and Fulvio Mastrogiovanni", }	article	shah2018detection	2018	30 - 49	10.1016/j.robot.2017.09.016	99		
2019-07-28	Meaning to Form: Measuring Systematicity as Information	Tiago Pimentel, Arya D. McCarthy, Damián Blasi, Brian Roark, Ryan Cotterell	Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics		Pimentel T, McCarthy AD, Blasi D, Roark B, Cotterell R. Meaning to Form: Measuring Systematicity as Information. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019 Jul (pp. 1751-1764).	pimentel-etal-2019-meaning	https://www.aclweb.org/anthology/P19-1171	A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram ‘gl’ have any systematic relationship to the meaning of words like ‘glisten’, ‘gleam’ and ‘glow’? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small—despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.	@inproceedings{pimentel2019meaning,     title = "Meaning to Form: {M}easuring Systematicity as Information",     author = "Pimentel, Tiago  and       McCarthy, Arya D.  and       Blasi, Damian  and       Roark, Brian  and       Cotterell, Ryan",     booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",     month = jul,     year = "2019",     address = "Florence, Italy",     publisher = "Association for Computational Linguistics",     url = "https://www.aclweb.org/anthology/P19-1171",     doi = "10.18653/v1/P19-1171",     pages = "1751--1764", }	inproceedings	pimentel2019meaning	2019	1751--1764	10.18653/v1/P19-1171			Nominated for the best paper award
2022-05-01	On the probability–quality paradox in language generation	Clara Meister, Gian Wiher, Tiago Pimentel, Ryan Cotterell	Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)		Clara Meister, Gian Wiher, Tiago Pimentel, and Ryan Cotterell. 2022. On the probability–quality paradox in language generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 36–45, Dublin, Ireland. Association for Computational Linguistics.	meister-et-al-2022a	https://aclanthology.org/2022.acl-short.5/	When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text—covering multiple tasks and common decoding strategies—suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.	@inproceedings{meister-etal-2022-high,     title = "On the probability{--}quality paradox in language generation",     author = "Meister, Clara  and       Wiher, Gian  and       Pimentel, Tiago  and       Cotterell, Ryan",     booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",     month = may,     year = "2022",     address = "Dublin, Ireland",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.acl-short.5",     doi = "10.18653/v1/2022.acl-short.5",     pages = "36--45",     abstract = "When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text{---}covering multiple tasks and common decoding strategies{---}suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.", }	inproceedings	meister-et-al-2022-high	2022	36--45	10.18653/v1/2022.acl-short.5			
2022-05-02	Analyzing Wrap-Up Effects through an Information-Theoretic Lens	Clara Meister, Tiago Pimentel, Thomas Clark, Ryan Cotterell, Roger Levy	Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)		Clara Meister, Tiago Pimentel, Thomas Clark, Ryan Cotterell, and Roger Levy. 2022. Analyzing Wrap-Up Effects through an Information-Theoretic Lens. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 20–28, Dublin, Ireland. Association for Computational Linguistics.	meister-et-al-2022b	https://aclanthology.org/2022.acl-short.3/	Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence–or even clause–is often omitted due to the confounding factors introduced by so-called “wrap-up effects,” which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these effects is limited. In this work, we attempt to learn more about these processes by looking for the existence–or absence–of a link between wrap-up effects and information theoretic quantities, such as word and context information content. We find that the information distribution of prior context is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs), which lends support to several prior hypotheses about the processes involved in wrap-up effects.	@inproceedings{meister-etal-2022-analyzing,     title = "Analyzing Wrap-Up Effects through an Information-Theoretic Lens",     author = "Meister, Clara  and       Pimentel, Tiago  and       Clark, Thomas  and       Cotterell, Ryan  and       Levy, Roger",     booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",     month = may,     year = "2022",     address = "Dublin, Ireland",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.acl-short.3",     doi = "10.18653/v1/2022.acl-short.3",     pages = "20--28",     abstract = "Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence{--}or even clause{--}is often omitted due to the confounding factors introduced by so-called {``}wrap-up effects,{''} which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these effects is limited. In this work, we attempt to learn more about these processes by looking for the existence{--}or absence{--}of a link between wrap-up effects and information theoretic quantities, such as word and context information content. We find that the information distribution of prior context is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs), which lends support to several prior hypotheses about the processes involved in wrap-up effects.", }	inproceedings	meister-et-al-2022-analyzing	2022	20--28	10.18653/v1/2022.acl-short.3			
2022-05-03	Probing for the Usage of Grammatical Number	Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, Ryan Cotterell	Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)		Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage of Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8818–8831, Dublin, Ireland. Association for Computational Linguistics.	lasri-et-al-2022	https://aclanthology.org/2022.acl-long.603/	A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious—i.e., the model might not rely on it when making predictions. In this paper, we try to find an encoding that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model’s representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Finally, we identify in which layers information about grammatical number is transferred from a noun to its head verb.	@inproceedings{lasri-etal-2022-probing,     title = "Probing for the Usage of Grammatical Number",     author = "Lasri, Karim  and       Pimentel, Tiago  and       Lenci, Alessandro  and       Poibeau, Thierry  and       Cotterell, Ryan",     booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",     month = may,     year = "2022",     address = "Dublin, Ireland",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.acl-long.603",     doi = "10.18653/v1/2022.acl-long.603",     pages = "8818--8831",     abstract = "A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious{---}i.e., the model might not rely on it when making predictions. In this paper, we try to find an encoding that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model{'}s representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Finally, we identify in which layers information about grammatical number is transferred from a noun to its head verb.", }	inproceedings	lasri-et-al-2022-probing	2022	8818--8831	10.18653/v1/2022.acl-long.603			
2022-06-01	UniMorph 4.0: Universal Morphology	Khuyagbaatar Batsuren, Omer Goldman, Salam Khalifa, Nizar Habash, Witold Kieraś, Gábor Bella, Brian Leonard, Garrett Nicolai, Kyle Gorman, Yustinus Ghanggo Ate, Maria Ryskina, Sabrina Mielke, Elena Budianskaya, Charbel El-Khaissi, Tiago Pimentel, Michael Gasser, William Abbott Lane, Mohit Raj, Matt Coler, Jaime Rafael Montoya Samame, Delio Siticonatzi Camaiteri, Esaú Zumaeta Rojas, Didier López Francis, Arturo Oncevay, Juan López Bautista, Gema Celeste Silva Villegas, Lucas Torroba Hennigen, Adam Ek, David Guriel, Peter Dirix, Jean-Philippe Bernardy, Andrey Scherbakov, Aziyana Bayyr-ool, Antonios Anastasopoulos, Roberto Zariquiey, Karina Sheifer, Sofya Ganieva, Hilaria Cruz, Ritván Karahóǧa, Stella Markantonatou, George Pavlidis, Matvey Plugaryov, Elena Klyachko, Ali Salehi, Candy Angulo, Jatayu Baxi, Andrew Krizhanovsky, Natalia Krizhanovskaya, Elizabeth Salesky, Clara Vania, Sardana Ivanova, Jennifer White, Rowan Hall Maudslay, Josef Valvoda, Ran Zmigrod, Paula Czarnowska, Irene Nikkarinen, Aelita Salchak, Brijesh Bhatt, Christopher Straughn, Zoey Liu, Jonathan North Washington, Yuval Pinter, Duygu Ataman, Marcin Wolinski, Totok Suhardijanto, Anna Yablonskaya, Niklas Stoehr, Hossep Dolatian, Zahroh Nuriah, Shyam Ratan, Francis M. Tyers, Edoardo M. Ponti, Grant Aiton, Aryaman Arora, Richard J. Hatcher, Ritesh Kumar, Jeremiah Young, Daria Rodionova, Anastasia Yemelina, Taras Andrushko, Igor Marchenko, Polina Mashkovtseva, Alexandra Serova, Emily Prud’hommeaux, Maria Nepomniashchaya, Fausto Giunchiglia, Eleanor Chodroff, Mans Hulden, Miikka Silfverberg, Arya D. McCarthy, David Yarowsky, Ryan Cotterell, Reut Tsarfaty, Ekaterina Vylomova	Proceedings of the Thirteenth Language Resources and Evaluation Conference		Khuyagbaatar Batsuren, Omer Goldman, Salam Khalifa, Nizar Habash, Witold Kieraś, Gábor Bella, Brian Leonard, Garrett Nicolai, Kyle Gorman, Yustinus Ghanggo Ate, Maria Ryskina, Sabrina Mielke, Elena Budianskaya, Charbel El-Khaissi, Tiago Pimentel, Michael Gasser, William Abbott Lane, Mohit Raj, Matt Coler, et al.. 2022. UniMorph 4.0: Universal Morphology. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 840–855, Marseille, France. European Language Resources Association.	batsuren-etal-2022-unimorph	https://aclanthology.org/2022.lrec-1.89/	The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological inflection tables for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation, and a type-level resource of annotated data in diverse languages realizing that schema. This paper presents the expansions and improvements on several fronts that were made in the last couple of years (since McCarthy et al. (2020)). Collaborative efforts by numerous linguists have added 66 new languages, including 24 endangered languages. We have implemented several improvements to the extraction pipeline to tackle some issues, e.g., missing gender and macrons information. We have amended the schema to use a hierarchical structure that is needed for morphological phenomena like multiple-argument agreement and case stacking, while adding some missing morphological features to make the schema more inclusive.In light of the last UniMorph release, we also augmented the database with morpheme segmentation for 16 languages. Lastly, this new release makes a push towards inclusion of derivational morphology in UniMorph by enriching the data and annotation schema with instances representing derivational processes from MorphyNet.	@inproceedings{batsuren-etal-2022-unimorph,     title = "{U}ni{M}orph 4.0: {U}niversal {M}orphology",     author = "Batsuren, Khuyagbaatar  and       Goldman, Omer  and       Khalifa, Salam  and       Habash, Nizar  and       Kiera{\'s}, Witold  and       Bella, G{\'a}bor  and       Leonard, Brian  and       Nicolai, Garrett  and       Gorman, Kyle  and       Ate, Yustinus Ghanggo  and       Ryskina, Maria  and       Mielke, Sabrina  and       Budianskaya, Elena  and       El-Khaissi, Charbel  and       Pimentel, Tiago  and       Gasser, Michael  and       Lane, William Abbott  and       Raj, Mohit  and       Coler, Matt  and       Samame, Jaime Rafael Montoya  and       Camaiteri, Delio Siticonatzi  and       Rojas, Esa{\'u} Zumaeta  and       L{\'o}pez Francis, Didier  and       Oncevay, Arturo  and       L{\'o}pez Bautista, Juan  and       Villegas, Gema Celeste Silva  and       Hennigen, Lucas Torroba  and       Ek, Adam  and       Guriel, David  and       Dirix, Peter  and       Bernardy, Jean-Philippe  and       Scherbakov, Andrey  and       Bayyr-ool, Aziyana  and       Anastasopoulos, Antonios  and       Zariquiey, Roberto  and       Sheifer, Karina  and       Ganieva, Sofya  and       Cruz, Hilaria  and       Karah{\'o}{\v{g}}a, Ritv{\'a}n  and       Markantonatou, Stella  and       Pavlidis, George  and       Plugaryov, Matvey  and       Klyachko, Elena  and       Salehi, Ali  and       Angulo, Candy  and       Baxi, Jatayu  and       Krizhanovsky, Andrew  and       Krizhanovskaya, Natalia  and       Salesky, Elizabeth  and       Vania, Clara  and       Ivanova, Sardana  and       White, Jennifer  and       Maudslay, Rowan Hall  and       Valvoda, Josef  and       Zmigrod, Ran  and       Czarnowska, Paula  and       Nikkarinen, Irene  and       Salchak, Aelita  and       Bhatt, Brijesh  and       Straughn, Christopher  and       Liu, Zoey  and       Washington, Jonathan North  and       Pinter, Yuval  and       Ataman, Duygu  and       Wolinski, Marcin  and       Suhardijanto, Totok  and       Yablonskaya, Anna  and       Stoehr, Niklas  and       Dolatian, Hossep  and       Nuriah, Zahroh  and       Ratan, Shyam  and       Tyers, Francis M.  and       Ponti, Edoardo M.  and       Aiton, Grant  and       Arora, Aryaman  and       Hatcher, Richard J.  and       Kumar, Ritesh  and       Young, Jeremiah  and       Rodionova, Daria  and       Yemelina, Anastasia  and       Andrushko, Taras  and       Marchenko, Igor  and       Mashkovtseva, Polina  and       Serova, Alexandra  and       Prud{'}hommeaux, Emily  and       Nepomniashchaya, Maria  and       Giunchiglia, Fausto  and       Chodroff, Eleanor  and       Hulden, Mans  and       Silfverberg, Miikka  and       McCarthy, Arya D.  and       Yarowsky, David  and       Cotterell, Ryan  and       Tsarfaty, Reut  and       Vylomova, Ekaterina",     booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",     month = jun,     year = "2022",     address = "Marseille, France",     publisher = "European Language Resources Association",     url = "https://aclanthology.org/2022.lrec-1.89",     pages = "840--855",     abstract = "The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological inflection tables for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation, and a type-level resource of annotated data in diverse languages realizing that schema. This paper presents the expansions and improvements on several fronts that were made in the last couple of years (since McCarthy et al. (2020)). Collaborative efforts by numerous linguists have added 66 new languages, including 24 endangered languages. We have implemented several improvements to the extraction pipeline to tackle some issues, e.g., missing gender and macrons information. We have amended the schema to use a hierarchical structure that is needed for morphological phenomena like multiple-argument agreement and case stacking, while adding some missing morphological features to make the schema more inclusive.In light of the last UniMorph release, we also augmented the database with morpheme segmentation for 16 languages. Lastly, this new release makes a push towards inclusion of derivational morphology in UniMorph by enriching the data and annotation schema with instances representing derivational processes from MorphyNet.", }	inproceedings	batsuren-etal-2022-unimorph	2022	840--855				
2022-07-01	Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective	Xin Xin, Tiago Pimentel, Alexandros Karatzoglou, Pengjie Ren, Konstantina Christakopoulou, Zhaochun Ren	Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval		Xin Xin, Tiago Pimentel, Alexandros Karatzoglou, Pengjie Ren, Konstantina Christakopoulou, and Zhaochun Ren. Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1347-1357. 2022.	xin-etal-2022-rethinking	https://dl.acm.org/doi/10.1145/3477495.3531714	Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective -- maximizing an user's reward per session -- it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the \emph{offline training challenge}. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of `errors' in the process. In the recommendation setting, though, we cannot afford the price of making `errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm -- namely Prompt-Based Reinforcement Learning (PRL) -- for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value -- with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: \emph{ Which item should be recommended given the prior interactions \& the prompted reward value}? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods. 	@inproceedings{xin-etal-2022-rethinking, author = {Xin, Xin and Pimentel, Tiago and Karatzoglou, Alexandros and Ren, Pengjie and Christakopoulou, Konstantina and Ren, Zhaochun}, title = {Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective}, year = {2022}, isbn = {9781450387323}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3477495.3531714}, doi = {10.1145/3477495.3531714}, abstract = {Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings.Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions \& the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.}, booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {1347–1357}, numpages = {11}, keywords = {recommender systems, session-based recommendation, next item recommendation, reinforcement learning}, location = {Madrid, Spain}, series = {SIGIR '22} }	inproceedings	xin-etal-2022-rethinking	2022	1347--1357	10.1145/3477495.3531714			
2022-12-01	The Architectural Bottleneck Principle	Tiago Pimentel, Josef Valvoda, Niklas Stoehr, Ryan Cotterell	Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing		Tiago Pimentel, Josef Valvoda, Niklas Stoehr, and Ryan Cotterell. 2022. The Architectural Bottleneck Principle. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11459–11472, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.	pimentel-etal-2022	https://aclanthology.org/2022.emnlp-main.788/	In this paper, we seek to measure how much information a component in a neural network could extract from the representations fed into it. Our work stands in contrast to prior probing work, most of which investigates how much information a model's representations contain. This shift in perspective leads us to propose a new principle for probing, the architectural bottleneck principle: In order to estimate how much information a given component could extract, a probe should look exactly like the component. Relying on this principle, we estimate how much syntactic information is available to transformers through our attentional probe, a probe that exactly resembles a transformer's self-attention head. Experimentally, we find that, in three models (BERT, ALBERT, and RoBERTa), a sentence's syntax tree is mostly extractable by our probe, suggesting these models have access to syntactic information while composing their contextual representations. Whether this information is actually used by these models, however, remains an open question. 	@inproceedings{pimentel-etal-2022-attentional,     title = "The Architectural Bottleneck Principle",     author = "Pimentel, Tiago  and       Valvoda, Josef  and       Stoehr, Niklas  and       Cotterell, Ryan",     booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",     month = dec,     year = "2022",     address = "Abu Dhabi, United Arab Emirates",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.emnlp-main.788",     pages = "11459--11472",     abstract = "", }	inproceedings	pimentel-etal-2022-architectural	2022	11459--11472				
2023-02-01	Locally Typical Sampling	Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell	Transactions of the Association for Computational Linguistics, Volume 11		Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2023. Locally Typical Sampling. Transactions of the Association for Computational Linguistics, 11:102–121.	meister-etal-2023-locally	https://aclanthology.org/2023.tacl-1.7/	Today’s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process—which allows for an information-theoretic analysis—can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.	@article{meister-etal-2023-locally,     title = "Locally Typical Sampling",     author = "Meister, Clara  and       Pimentel, Tiago  and       Wiher, Gian  and       Cotterell, Ryan",     journal = "Transactions of the Association for Computational Linguistics",     volume = "11",     year = "2023",     address = "Cambridge, MA",     publisher = "MIT Press",     url = "https://aclanthology.org/2023.tacl-1.7",     doi = "10.1162/tacl_a_00536",     pages = "102--121",     abstract = "Today{'}s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process{---}which allows for an information-theoretic analysis{---}can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.", }	article	meister-etal-2023-locally	2023	102--121	10.1162/tacl_a_00536	11		
2023-02-02	Naturalistic Causal Probing for Morpho-Syntax	Afra Amini, Tiago Pimentel, Clara Meister, Ryan Cotterell	Transactions of the Association for Computational Linguistics, Volume 11		Afra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell. 2023. Naturalistic Causal Probing for Morpho-Syntax. Transactions of the Association for Computational Linguistics, 11:384–403.	amini-etal-2023-naturalistic	https://aclanthology.org/2023.tacl-1.23/	Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. However, there is still a lack of understanding of the limitations and weaknesses of various types of probes. In this work, we suggest a strategy for input-level intervention on naturalistic sentences. Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models. We apply our naturalistic causal probing framework to analyze the effects of grammatical gender and number on contextualized representations extracted from three pre-trained models in Spanish, the multilingual versions of BERT, RoBERTa, and GPT-2. Our experiments suggest that naturalistic interventions lead to stable estimates of the causal effects of various linguistic properties. Moreover, our experiments demonstrate the importance of naturalistic causal probing when analyzing pre-trained models. https://github.com/rycolab/naturalistic-causal-probing	@article{amini-etal-2023-naturalistic,     title = "Naturalistic Causal Probing for Morpho-Syntax",     author = "Amini, Afra  and       Pimentel, Tiago  and       Meister, Clara  and       Cotterell, Ryan",     journal = "Transactions of the Association for Computational Linguistics",     volume = "11",     year = "2023",     address = "Cambridge, MA",     publisher = "MIT Press",     url = "https://aclanthology.org/2023.tacl-1.23",     doi = "10.1162/tacl_a_00554",     pages = "384--403",     abstract = "Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. However, there is still a lack of understanding of the limitations and weaknesses of various types of probes. In this work, we suggest a strategy for input-level intervention on naturalistic sentences. Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models. We apply our naturalistic causal probing framework to analyze the effects of grammatical gender and number on contextualized representations extracted from three pre-trained models in Spanish, the multilingual versions of BERT, RoBERTa, and GPT-2. Our experiments suggest that naturalistic interventions lead to stable estimates of the causal effects of various linguistic properties. Moreover, our experiments demonstrate the importance of naturalistic causal probing when analyzing pre-trained models. https://github.com/rycolab/naturalistic-causal-probing", }	article	amini-etal-2023-naturalistic	2023	384--403	10.1162/tacl_a_00554	11		
2023-05-01	On the Intersection of Context-Free and Regular Languages	Clemente Pasti, Andreas Opedal, Tiago Pimentel, Tim Vieira, Jason Eisner, Ryan Cotterell	Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics		Clemente Pasti, Andreas Opedal, Tiago Pimentel, Tim Vieira, Jason Eisner, and Ryan Cotterell. 2023. On the Intersection of Context-Free and Regular Languages. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 737–749, Dubrovnik, Croatia. Association for Computational Linguistics.	pasti-etal-2023-intersection	https://aclanthology.org/2023.eacl-main.52/	The Bar-Hillel construction is a classic result in formal language theory. It shows, by a simple construction, that the intersection of a context-free language and a regular language is itself context-free. In the construction, the regular language is specified by a finite-state automaton. However, neither the original construction (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle finite-state automata with ε-arcs. While it is possible to remove ε-arcs from a finite-state automaton efficiently without modifying the language, such an operation modifies the automaton’s set of paths. We give a construction that generalizes the Bar- Hillel in the case the desired automaton has ε-arcs, and further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction.	@inproceedings{pasti-etal-2023-intersection,     title = "On the Intersection of Context-Free and Regular Languages",     author = "Pasti, Clemente  and       Opedal, Andreas  and       Pimentel, Tiago  and       Vieira, Tim  and       Eisner, Jason  and       Cotterell, Ryan",     booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",     month = may,     year = "2023",     address = "Dubrovnik, Croatia",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.eacl-main.52",     pages = "737--749",     abstract = "The Bar-Hillel construction is a classic result in formal language theory. It shows, by a simple construction, that the intersection of a context-free language and a regular language is itself context-free. In the construction, the regular language is specified by a finite-state automaton. However, neither the original construction (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle finite-state automata with ε-arcs. While it is possible to remove ε-arcs from a finite-state automaton efficiently without modifying the language, such an operation modifies the automaton{'}s set of paths. We give a construction that generalizes the Bar- Hillel in the case the desired automaton has ε-arcs, and further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction.", }	inproceedings	pasti-etal-2023-intersection	2023	737--749				
2023-05-02	On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation	Tiago Pimentel, Clara Meister, Ryan Cotterell	The Eleventh International Conference on Learning Representations		Tiago Pimentel, Clara Isabel Meister, and Ryan Cotterell. "On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation." The Eleventh International Conference on Learning Representations. 2022.	pimentel-etal-2023-usefulness	https://openreview.net/forum?id=bvpkw7UIRdU	A good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality.  Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is  the recently proposed Mauve. In theory, Mauve measures an information-theoretic divergence between two probability distributions over strings: one representing the language generator under evaluation; the other representing the true natural language distribution. Mauve's authors argue that its success comes from the qualitative properties of their proposed divergence.  Yet in practice, as this divergence is uncomputable, Mauve approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pretrained language model's embeddings. As we show, however, this is not a tight approximation---in either theory or practice. This begs the question: why does Mauve work so well? In this work, we show that \mauve was right for the wrong reasons, and that its newly proposed divergence is not necessary for its high performance. In fact, classical divergences paired with its proposed cluster-based approximation may actually serve as better evaluation metrics. We finish the paper with a probing analysis; this analysis leads us to conclude that---by encoding syntactic- and coherence-level features of text, while ignoring surface-level features---such cluster-based approximations to string distributions may simply be better for evaluating state-of-the-art language generators.	@inproceedings{pimentel2023on, title={On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation}, author={Tiago Pimentel and Clara Isabel Meister and Ryan Cotterell}, booktitle={The Eleventh International Conference on Learning Representations }, year={2023}, url={https://openreview.net/forum?id=bvpkw7UIRdU} }	inproceedings	pimentel-etal-2023-usefulness	2023					
2023-06-04	Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models	Karolina Stańczak, Sagnik Ray Choudhury, Tiago Pimentel, Ryan Cotterell, Isabelle Augenstein	PLOS One		Karolina Stańczak, Sagnik Ray Choudhury, Tiago Pimentel, Ryan Cotterell, and Isabelle Augenstein. Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models. PLOS One (2023).	stanczak-etal-2023-quantifying	https://arxiv.org/abs/2104.07505	While the prevalence of large pre-trained language models has led to significant improvements in the performance of NLP systems, recent research has demonstrated that these models inherit societal biases extant in natural language. In this paper, we explore a simple method to probe pre-trained language models for gender bias, which we use to effect a multi-lingual study of gender bias towards politicians. We construct a dataset of 250k politicians from most countries in the world and quantify adjective and verb usage around those politicians' names as a function of their gender. We conduct our study in 7 languages across 6 different language modeling architectures. Our results demonstrate that stance towards politicians in pre-trained language models is highly dependent on the language used. Finally, contrary to previous findings, our study suggests that larger language models do not tend to be significantly more gender-biased than smaller ones. 		article	stanczak-etal-2023-quantifying	2023					
2023-07-01	A Natural Bias for Language Generation Models	Clara Meister, Wojciech Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell, Adhiguna Kuncoro	Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)		Clara Meister, Wojciech Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell, and Adhiguna Kuncoro. 2023. A Natural Bias for Language Generation Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 243–255, Toronto, Canada. Association for Computational Linguistics.	meister-etal-2023-natural	https://aclanthology.org/2023.acl-short.22/	After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a heuristic raises the question: Can we initialise our models with this behaviour and save precious compute resources and model capacity? Here we show that we can effectively endow standard neural language generation models with a separate module that reflects unigram frequency statistics as prior knowledge, simply by initialising the bias term in a model’s final linear layer with the log-unigram distribution. We use neural machine translation as a test bed for this simple technique and observe that it: (i) improves learning efficiency; (ii) achieves better overall performance; and perhaps most importantly (iii) appears to disentangle strong frequency effects by encouraging the model to specialise in non-frequency-related aspects of language.	@inproceedings{meister-etal-2023-natural,     title = "A Natural Bias for Language Generation Models",     author = "Meister, Clara  and       Stokowiec, Wojciech  and       Pimentel, Tiago  and       Yu, Lei  and       Rimell, Laura  and       Kuncoro, Adhiguna",     booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",     month = jul,     year = "2023",     address = "Toronto, Canada",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.acl-short.22",     pages = "243--255",     abstract = "After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a heuristic raises the question: Can we initialise our models with this behaviour and save precious compute resources and model capacity? Here we show that we can effectively endow standard neural language generation models with a separate module that reflects unigram frequency statistics as prior knowledge, simply by initialising the bias term in a model{'}s final linear layer with the log-unigram distribution. We use neural machine translation as a test bed for this simple technique and observe that it: (i) improves learning efficiency; (ii) achieves better overall performance; and perhaps most importantly (iii) appears to disentangle strong frequency effects by encouraging the model to specialise in non-frequency-related aspects of language.", }	inproceedings	meister-etal-2023-natural	2023	243--255				
2023-07-02	A Measure-Theoretic Characterization of Tight Language Models	Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, Ryan Cotterell	Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)		Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, and Ryan Cotterell. 2023. A Measure-Theoretic Characterization of Tight Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9744–9770, Toronto, Canada. Association for Computational Linguistics.	du-etal-2023-measure	https://aclanthology.org/2023.acl-long.543/	Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.	@inproceedings{du-etal-2023-measure,     title = "A Measure-Theoretic Characterization of Tight Language Models",     author = "Du, Li  and       Torroba Hennigen, Lucas  and       Pimentel, Tiago  and       Meister, Clara  and       Eisner, Jason  and       Cotterell, Ryan",     booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",     month = jul,     year = "2023",     address = "Toronto, Canada",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.acl-long.543",     pages = "9744--9770",     abstract = "Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can {``}leak{''} onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.", }	inproceedings	du-etal-2023-measure	2023	9744--9770				
2023-07-03	Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation	Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar	Findings of the Association for Computational Linguistics: ACL 2023		Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 12284–12314, Toronto, Canada. Association for Computational Linguistics.	mosbach-etal-2023-shot	https://aclanthology.org/2023.findings-acl.779/	Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations.Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.	@inproceedings{mosbach-etal-2023-shot,     title = "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation",     author = "Mosbach, Marius  and       Pimentel, Tiago  and       Ravfogel, Shauli  and       Klakow, Dietrich  and       Elazar, Yanai",     booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",     month = jul,     year = "2023",     address = "Toronto, Canada",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.findings-acl.779",     pages = "12284--12314",     abstract = "Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations.Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.", }	inproceedings	mosbach-etal-2023-shot	2023	12284--12314				
2023-07-04	On the Efficacy of Sampling Adapters	Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan G. Wilcox, Ryan Cotterell	Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)		Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan Wilcox, and Ryan Cotterell. 2023. On the Efficacy of Sampling Adapters. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1437–1455, Toronto, Canada. Association for Computational Linguistics.	meister-etal-2023-efficacy	https://aclanthology.org/2023.acl-long.80/	Sampling-based decoding strategies are widely employed for generating text from probabilistic models, yet standard ancestral sampling often results in text that is degenerate or incoherent. To alleviate this issue, various modifications to a model’s sampling distribution, such as top-p or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the token-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of distribution quality (such as perplexity), we find that several precision-emphasizing measures indeed indicate that sampling adapters can lead to probability distributions more aligned with the true distribution. Further, these measures correlate with higher sequence-level quality scores, specifically, Mauve.	@inproceedings{meister-etal-2023-efficacy,     title = "On the Efficacy of Sampling Adapters",     author = "Meister, Clara  and       Pimentel, Tiago  and       Malagutti, Luca  and       Wilcox, Ethan  and       Cotterell, Ryan",     booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",     month = jul,     year = "2023",     address = "Toronto, Canada",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.acl-long.80",     pages = "1437--1455",     abstract = "Sampling-based decoding strategies are widely employed for generating text from probabilistic models, yet standard ancestral sampling often results in text that is degenerate or incoherent. To alleviate this issue, various modifications to a model{'}s sampling distribution, such as top-p or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the token-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of distribution quality (such as perplexity), we find that several precision-emphasizing measures indeed indicate that sampling adapters can lead to probability distributions more aligned with the true distribution. Further, these measures correlate with higher sequence-level quality scores, specifically, Mauve.", }	inproceedings	meister-etal-2023-efficacy	2023	1437--1455				
2023-08-01	A Cross-Linguistic Pressure for Uniform Information Density in Word Order	Thomas Hikaru Clark, Clara Meister, Tiago Pimentel, Michael Hahn, Ryan Cotterell, Richard Futrell, Roger Levy	Transactions of the Association for Computational Linguistics		Thomas Hikaru Clark, Clara Meister, Tiago Pimentel, Michael Hahn, Ryan Cotterell, Richard Futrell, and Roger Levy. A Cross-Linguistic Pressure for Uniform Information Density in Word Order. Transactions of the Association for Computational Linguistics (2023).	clark-etal-2023-crosslinguistic	https://aclanthology.org/2023.tacl-1.59/	While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: the uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages. 	@article{clark-etal-2023-cross,     title = "A Cross-Linguistic Pressure for {U}niform {I}nformation {D}ensity in Word Order",     author = "Clark, Thomas Hikaru  and       Meister, Clara  and       Pimentel, Tiago  and       Hahn, Michael  and       Cotterell, Ryan  and       Futrell, Richard  and       Levy, Roger",     journal = "Transactions of the Association for Computational Linguistics",     volume = "11",     year = "2023",     address = "Cambridge, MA",     publisher = "MIT Press",     url = "https://aclanthology.org/2023.tacl-1.59",     doi = "10.1162/tacl_a_00589",     pages = "1048--1065", }	article	clark-etal-2023-crosslinguistic	2023					
2023-08-02	Testing the Predictions of Surprisal Theory in 11 Languages	Ethan G. Wilcox, Tiago Pimentel, Clara Meister, Ryan Cotterell, Roger P. Levy	Transactions of the Association for Computational Linguistics		Ethan G. Wilcox, Tiago Pimentel, Clara Meister, Ryan Cotterell, and Roger P. Levy. Testing the Predictions of Surprisal Theory in 11 Languages. Transactions of the Association for Computational Linguistics (2023).	wilcox-etal-2023-testing	https://arxiv.org/abs/2307.03667	A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, is predictive of reading times; (iii) and whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to-date between information theory and incremental language processing across languages. 	@article{wilcox-etal-2023-testing,     author = {Wilcox, Ethan G. and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan and Levy, Roger P.},     title = "Testing the Predictions of Surprisal Theory in 11 Languages",     journal = {Transactions of the Association for Computational Linguistics},     volume = {11},     pages = {1451-1470},     year = {2023},     month = {12},     issn = {2307-387X},     doi = {10.1162/tacl_a_00612},     url = {https://doi.org/10.1162/tacl\_a\_00612},     eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00612/2196877/tacl\_a\_00612.pdf}, }	article	wilcox-etal-2023-testing	2023					
2023-08-03	On the Effect of Anticipation on Reading Times	Tiago Pimentel, Clara Meister, Ethan G. Wilcox, Roger Levy, Ryan Cotterell	Transactions of the Association for Computational Linguistics		Tiago Pimentel, Clara Meister, Ethan G. Wilcox, Roger Levy, and Ryan Cotterell. On the Effect of Anticipation on Reading Times. Transactions of the Association for Computational Linguistics (2023).	pimentel-etal-2023-effect	https://arxiv.org/abs/2211.14301	Over the past two decades, numerous studies have demonstrated how less predictable (i.e., higher surprisal) words take more time to read. In general, these studies have implicitly assumed the reading process is purely responsive: Readers observe a new word and allocate time to process it as required. We argue that prior results are also compatible with a reading process that is at least partially anticipatory: Readers could make predictions about a future word and allocate time to process it based on their expectation. In this work, we operationalize this anticipation as a word's contextual entropy. We assess the effect of anticipation on reading by comparing how well surprisal and contextual entropy predict reading times on four naturalistic reading datasets: two self-paced and two eye-tracking. Experimentally, across datasets and analyses, we find substantial evidence for effects of contextual entropy over surprisal on a word's reading time (RT): in fact, entropy is sometimes better than surprisal in predicting a word's RT. Spillover effects, however, are generally not captured by entropy, but only by surprisal. Further, we hypothesize four cognitive mechanisms through which contextual entropy could impact RTs -- three of which we are able to design experiments to analyze. Overall, our results support a view of reading that is not just responsive, but also anticipatory. 	@article{pimentel-etal-2023-effect,     author = {Pimentel, Tiago and Meister, Clara and Wilcox, Ethan G. and Levy, Roger P. and Cotterell, Ryan},     title = "{On the Effect of Anticipation on Reading Times}",     journal = {Transactions of the Association for Computational Linguistics},     volume = {11},     pages = {1624-1642},     year = {2023},     month = {12},     issn = {2307-387X},     doi = {10.1162/tacl_a_00603},     url = {https://doi.org/10.1162/tacl\_a\_00603},     eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00603/2196892/tacl\_a\_00603.pdf}, }    	article	pimentel-etal-2023-effect	2023					
2023-12-09	Revisiting the Optimality of Word Lengths	Tiago Pimentel, Ethan G. Wilcox, Clara Meister, Kyle Mahowald, Ryan Cotterell	Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing		Tiago Pimentel, Clara Meister, Ethan G. Wilcox, Kyle Mahowald, and Ryan Cotterell. 2023. Revisiting the Optimality of Word Lengths. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2240–2255, Singapore. Association for Computational Linguistics.	pimentel-etal-2023-revisiting	https://aclanthology.org/2023.emnlp-main.137/	Zipf (1935) posited that wordforms are optimized to minimize utterances’ communicative costs. Under the assumption that cost is given by an utterance’s length, he supported this claim by showing that words’ lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance’s information rate and channel capacity, which we dub the channel capacity hypothesis (CCH) here. Following this logic, they then proposed that a word’s length should be proportional to the expected value of its surprisal (negative log-probability in context). In this work, we show that Piantadosi et al.’s derivation does not minimize CCH’s cost, but rather a lower bound, which we term CCH-lower. We propose a novel derivation, suggesting an improved way to minimize CCH’s cost. Under this method, we find that a language’s word lengths should instead be proportional to the surprisal’s expectation plus its variance-to-mean ratio. Experimentally, we compare these three communicative cost functions: Zipf’s, CCH-lower , and CCH. Across 13 languages and several experimental settings, we find that length is better predicted by frequency than either of the other hypotheses. In fact, when surprisal’s expectation, or expectation plus variance-to-mean ratio, is estimated using better language models, it leads to worse word length predictions. We take these results as evidence that Zipf’s longstanding hypothesis holds. 	@inproceedings{pimentel-etal-2023-revisiting,     title = "Revisiting the Optimality of Word Lengths",     author = "Pimentel, Tiago  and       Meister, Clara  and       Wilcox, Ethan  and       Mahowald, Kyle  and       Cotterell, Ryan",     editor = "Bouamor, Houda  and       Pino, Juan  and       Bali, Kalika",     booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",     month = dec,     year = "2023",     address = "Singapore",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.emnlp-main.137",     doi = "10.18653/v1/2023.emnlp-main.137",     pages = "2240--2255", }	inproceedings	pimentel-etal-2023-revisiting	2023	2240–2255	10.18653/v1/2023.emnlp-main.137			Won an Outstanding Paper award in the Linguistic Theories, Cognitive Modeling, and Psycholinguistics track
2023-12-08	Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages	Ethan G. Wilcox, Clara Meister, Ryan Cotterell, Tiago Pimentel	Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing		Ethan G. Wilcox, Clara Meister, Ryan Cotterell, and Tiago Pimentel. 2023. Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7503–7511, Singapore. Association for Computational Linguistics.	wilcox-etal-2023-language	https://aclanthology.org/2023.emnlp-main.466/	Surprisal theory (Hale, 2001; Levy, 2008) posits that a word’s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word’s ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading behavior—a conjecture we dub the quality–power (QP) hypothesis. Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations. In this work, we conduct a systematic crosslinguistic assessment of the QP hypothesis. We train LMs from scratch on small- and medium-sized datasets from 13 languages (across five language families) and assess their ability to predict eye tracking data. We find correlations between LM quality and power in eleven of these thirteen languages, suggesting that, within the range of model classes and sizes tested, better language models are indeed better predictors of human language processing behaviors. 	@inproceedings{wilcox-etal-2023-language,     title = "Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages",     author = "Wilcox, Ethan  and       Meister, Clara  and       Cotterell, Ryan  and       Pimentel, Tiago",     editor = "Bouamor, Houda  and       Pino, Juan  and       Bali, Kalika",     booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",     month = dec,     year = "2023",     address = "Singapore",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.emnlp-main.466",     doi = "10.18653/v1/2023.emnlp-main.466",     pages = "7503--7511", }	inproceedings	wilcox-etal-2023-language	2023	7503–7511	10.18653/v1/2023.emnlp-main.466			Won an Outstanding Paper award in the Linguistic Theories, Cognitive Modeling, and Psycholinguistics track
2023-12-07	Quantifying the redundancy between prosody and text	Lukas Wolf, Tiago Pimentel, Evelina Fedorenko, Ryan Cotterell, Alex Warstadt, Ethan G. Wilcox, Tamar Regev	Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing		Lukas Wolf, Tiago Pimentel, Evelina Fedorenko, Ryan Cotterell, Alex Warstadt, Ethan G. Wilcox, and Tamar Regev. 2023. Quantifying the redundancy between prosody and text. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9765–9784, Singapore. Association for Computational Linguistics.	wolf-etal-2023-quantifying	https://aclanthology.org/2023.emnlp-main.606/	Prosody—the suprasegmental component of speech, including pitch, loudness, and tempo—carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word’s prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features. 	@inproceedings{wolf-etal-2023-quantifying,     title = "Quantifying the redundancy between prosody and text",     author = "Wolf, Lukas  and       Pimentel, Tiago  and       Fedorenko, Evelina  and       Cotterell, Ryan  and       Warstadt, Alex  and       Wilcox, Ethan  and       Regev, Tamar",     editor = "Bouamor, Houda  and       Pino, Juan  and       Bali, Kalika",     booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",     month = dec,     year = "2023",     address = "Singapore",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.emnlp-main.606",     doi = "10.18653/v1/2023.emnlp-main.606",     pages = "9765--9784", }	inproceedings	wolf-etal-2023-quantifying	2023	9765–9784	10.18653/v1/2023.emnlp-main.606			
2023-12-06	An Exploration of Left-Corner Transformations	Andreas Opedal, Eleftheria Tsipidi, Tiago Pimentel, Ryan Cotterell, Tim Vieira	Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing		Andreas Opedal, Eleftheria Tsipidi, Tiago Pimentel, Ryan Cotterell, and Tim Vieira. 2023. An Exploration of Left-Corner Transformations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13393–13427, Singapore. Association for Computational Linguistics.	opedal-etal-2023-generalized	https://aclanthology.org/2023.emnlp-main.827/	The left-corner transformation (Rosenkrantz and Lewis, 1970) is used to remove left recursion from context-free grammars, which is an important step towards making the grammar parsable top-down with simple techniques. This paper generalizes prior left-corner transformations to support semiring-weighted production rules and to provide finer-grained control over which left corners may be moved. Our generalized left-corner transformation (GLCT) arose from unifying the left-corner transformation and speculation transformation (Eisner and Blatz, 2007), originally for logic programming. Our new transformation and speculation define equivalent weighted languages. Yet, their derivation trees are structurally different in an important way: GLCT replaces left recursion with right recursion, and speculation does not. We also provide several technical results regarding the formal relationships between the outputs of GLCT, speculation, and the original grammar. Lastly, we empirically investigate the efficiency of GLCT for left-recursion elimination from grammars of nine languages. Code: https://github.com/rycolab/left-corner	@inproceedings{opedal-etal-2023-exploration,     title = "An Exploration of Left-Corner Transformations",     author = "Opedal, Andreas  and       Tsipidi, Eleftheria  and       Pimentel, Tiago  and       Cotterell, Ryan  and       Vieira, Tim",     editor = "Bouamor, Houda  and       Pino, Juan  and       Bali, Kalika",     booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",     month = dec,     year = "2023",     address = "Singapore",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2023.emnlp-main.827",     doi = "10.18653/v1/2023.emnlp-main.827",     pages = "13393--13427", }	inproceedings	opedal-etal-2023-generalized	2023	13393–13427	10.18653/v1/2023.emnlp-main.827			
2023-10-19	A taxonomy and review of generalisation research in NLP	Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, Zhijing Jin	Nature Machine Intelligence		Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, and Zhijing Jin. 2024. A taxonomy and review of generalisation research in NLP. In Nature Machine Intelligence, pages 1161--1174.	hupkes2023taxonomy	https://doi.org/10.1038/s42256-023-00729-y	The ability to generalize well is one of the primary desiderata for models of natural language processing (NLP), but what ‘good generalization’ entails and how it should be evaluated is not well understood. In this Analysis we present a taxonomy for characterizing and understanding generalization research in NLP. The proposed taxonomy is based on an extensive literature review and contains five axes along which generalization studies can differ: their main motivation, the type of generalization they aim to solve, the type of data shift they consider, the source by which this data shift originated, and the locus of the shift within the NLP modelling pipeline. We use our taxonomy to classify over 700 experiments, and we use the results to present an in-depth analysis that maps out the current state of generalization research in NLP and make recommendations for which areas deserve attention in the future.  	@article{hupkes2023taxonomy,         author = {Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and Ulmer, Dennis and Schottmann, Florian and Batsuren, Khuyagbaatar and Sun, Kaiser and Sinha, Koustuv and Khalatbari, Leila and Ryskina, Maria and Frieske, Rita and Cotterell, Ryan and Jin, Zhijing},         doi = {10.1038/s42256-023-00729-y},         id = {Hupkes2023},         isbn = {2522-5839},         journal = {Nature Machine Intelligence},         number = {10},         pages = {1161--1174},         title = {A taxonomy and review of generalization research in NLP},         url = {https://doi.org/10.1038/s42256-023-00729-y},         volume = {5},         year = {2023}, } 	article	hupkes2023taxonomy	2023	1161--1174	10.1038/s42256-023-00729-y	5	10	
2024-02-29	Large-scale evidence for logarithmic effects of word predictability on reading time	Cory Shain, Clara Meister, Tiago Pimentel, Ryan Cotterell, Roger Levy	Proceedings of the National Academy of Sciences		Cory Shain, Clara Meister, Tiago Pimentel, Ryan Cotterell, and Roger Levy. 2024. Large-scale evidence for logarithmic effects of word predictability on reading time. In Proceedings of the National Academy of Sciences.	shain2024largescale	https://www.pnas.org/doi/abs/10.1073/pnas.2307876121	During real-time language comprehension, our minds rapidly decode complex meanings from sequences of words. The difficulty of doing so is known to be related to words’ contextual predictability, but what cognitive processes do these predictability effects reflect? In one view, predictability effects reflect facilitation due to anticipatory processing of words that are predictable from context. This view predicts a linear effect of predictability on processing demand. In another view, predictability effects reflect the costs of probabilistic inference over sentence interpretations. This view predicts either a logarithmic or a superlogarithmic effect of predictability on processing demand, depending on whether it assumes pressures toward a uniform distribution of information over time. The empirical record is currently mixed. Here, we revisit this question at scale: We analyze six reading datasets, estimate next-word probabilities with diverse statistical language models, and model reading times using recent advances in nonlinear regression. Results support a logarithmic effect of word predictability on processing difficulty, which favors probabilistic inference as a key component of human language processing.	@article{shain2024largescale,   author = {Cory Shain  and Clara Meister  and Tiago Pimentel  and Ryan Cotterell  and Roger Levy },   title = {Large-scale evidence for logarithmic effects of word predictability on reading time},   journal = {Proceedings of the National Academy of Sciences},   volume = {121},   number = {10},   pages = {e2307876121},   year = {2024},   doi = {10.1073/pnas.2307876121},   URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2307876121},   eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2307876121} }  	article	shain2024largescale	2024	e2307876121	10.1073/pnas.2307876121	121	10	
2024-03-01	An Information-theoretic Analysis of Regressions in Naturalistic Reading	Ethan G. Wilcox, Tiago Pimentel, Clara Meister, Ryan Cotterell	Cognition		Ethan G. Wilcox, Tiago Pimentel, Clara Meister, and Ryan Cotterell. 2024. An Information-theoretic Analysis of Regressions in Naturalistic Reading. In Cognition.	wilcox2024informationtheoretic	https://doi.org/10.31234/osf.io/3qf9a	Regressions, or backward saccades, are common during reading, accounting for between 5% and 20% of all saccades. And yet, relatively little is known about what causes them. We provide an information-theoretic operationalization for two previous qualitative hypotheses about regressions, which we dub reactivation and reanalysis. We argue that these hypotheses make different predictions about the pointwise mutual information or PMI between a regression’s source and target. Intuitively, the PMI between two words measures how much more (or less) likely one word is to be present given the other. On one hand, the reactivation hypothesis predicts that regressions occur between words that are associated, implying high positive values of PMI. On the other hand, the reanalysis hypothesis predicts that regressions should occur between words that are disassociated with each other, implying negative, low values of PMI. As a second theoretical contribution, we expand on previous theories by considering not only PMI but also expected values of PMI, E[PMI], where the expectation is taken over all possible realizations of the regression's target. The rationale for this is that language processing involves making inferences under uncertainty, and readers may be uncertain about what they have read, especially if a previous word was skipped. To test both theories, we use contemporary language models to estimate PMI-based statistics over word pairs in three corpora of eye tracking data in English, as well as in six languages across three language families (Indo-European, Uralic, and Turkic). Our results are consistent across languages and models tested: Positive values of PMI and E[PMI] consistently help to predict the patterns of regressions during reading, whereas negative values of PMI and E[PMI] do not. Our information-theoretic interpretation increases the predictive scope of both theories and our studies present the first systematic crosslinguistic analysis of regressions in the literature. Our results support the reactivation hypothesis and, more broadly, they expand the number of language processing behaviors that can be linked to information-theoretic principles.	@article{wilcox2024informationtheoretic,  title={An Information-Theoretic Analysis of Targeted Regressions during Reading},  url={osf.io/preprints/psyarxiv/3qf9a},  DOI={10.31234/osf.io/3qf9a},  journal={Cognition},  author={Wilcox, Ethan G. and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan},  year={2024}, }	article	wilcox2024informationtheoretic	2024					